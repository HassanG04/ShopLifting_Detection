{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4eda2dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Imports & Config ---\n",
    "import os, glob, math, random, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "CLASS_NAMES = [\"non_shop_lifters\", \"shop_lifters\"]\n",
    "MODELS_DIR = Path(\"models\"); MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training knobs (start here; tune later)\n",
    "T = 8                 # frames per clip (try 8–12)\n",
    "IMG_SIZE = 224       # 224 is good for MobileNet\n",
    "BATCH_SIZE = 4       # clips are heavy; 4–8 on 8GB VRAM\n",
    "EPOCHS_WARMUP = 6\n",
    "EPOCHS_FT = 8\n",
    "LR_WARMUP = 3e-4\n",
    "LR_FT = 1e-4\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c34f8f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found videos: 855 | per class: Counter({0: 531, 1: 324})\n",
      "Split | train: 598 val: 128 test: 129\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Index dataset & split by video ---\n",
    "video_records = []\n",
    "for ci, cname in enumerate(CLASS_NAMES):\n",
    "    vids = sorted(glob.glob(str((DATA_DIR / cname / \"*.mp4\").resolve())))\n",
    "    for v in vids:\n",
    "        video_records.append((v, ci))\n",
    "print(f\"Found videos: {len(video_records)} | per class:\",\n",
    "      Counter([c for _, c in video_records]))\n",
    "\n",
    "# Stratified split by class\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_rec, tmp = train_test_split(\n",
    "    video_records, test_size=0.30, random_state=SEED,\n",
    "    stratify=[c for _, c in video_records]\n",
    ")\n",
    "val_rec, test_rec = train_test_split(\n",
    "    tmp, test_size=0.50, random_state=SEED,\n",
    "    stratify=[c for _, c in tmp]\n",
    ")\n",
    "\n",
    "print(\"Split | train:\", len(train_rec), \"val:\", len(val_rec), \"test:\", len(test_rec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e4c4b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Canonical motion sampler (shared by train + app) ---\n",
    "def sample_motion_frames(video_path, k=T, img_size=IMG_SIZE):\n",
    "    import cv2, numpy as np\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened(): return None\n",
    "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    ok, prev = cap.read()\n",
    "    if not ok or n <= 1:\n",
    "        cap.release(); return None\n",
    "    prev_g = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "    frames = [prev]; diffs = []\n",
    "\n",
    "    for i in range(1, n):\n",
    "        ok, f = cap.read()\n",
    "        if not ok: break\n",
    "        frames.append(f)\n",
    "        g = cv2.cvtColor(f, cv2.COLOR_BGR2GRAY)\n",
    "        diffs.append((i, float(cv2.absdiff(g, prev_g).mean())))\n",
    "        prev_g = g\n",
    "    cap.release()\n",
    "    if not diffs: return None\n",
    "\n",
    "    k = min(k, len(diffs))\n",
    "    top = sorted(sorted(diffs, key=lambda x: x[1], reverse=True)[:k], key=lambda x: x[0])\n",
    "    idxs = [i for i,_ in top]\n",
    "\n",
    "    out = []\n",
    "    for i in idxs:\n",
    "        f = frames[i]\n",
    "        f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "        f = cv2.resize(f, (img_size, img_size), interpolation=cv2.INTER_AREA)\n",
    "        out.append(f.astype(\"float32\"))\n",
    "    clip = np.stack(out, axis=0) if out else None\n",
    "    if clip is None: return None\n",
    "    if clip.shape[0] < T:\n",
    "        pad = np.repeat(clip[-1:], T - clip.shape[0], axis=0)\n",
    "        clip = np.concatenate([clip, pad], axis=0)\n",
    "    return clip  # (T,H,W,3) float32 0..255\n",
    "\n",
    "# --- Canonical frames->tensor normalization (shared) ---\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "to_tensor = transforms.ToTensor()\n",
    "normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "def clip_to_tensor(clip_hwc_uint8_or_float):\n",
    "    import numpy as np\n",
    "    frames = []\n",
    "    for f in clip_hwc_uint8_or_float:\n",
    "        x = to_tensor(f.astype(np.uint8))      # [0,1], CHW\n",
    "        x = normalize(x)                       # ImageNet norm\n",
    "        frames.append(x)\n",
    "    return torch.stack(frames, dim=0)          # (T,3,H,W)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4514c79f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(598, 128)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Cell 4: Dataset that yields (T,3,H,W) normalized clips ---\n",
    "class ClipDataset(Dataset):\n",
    "    def __init__(self, records, t=T, img_size=IMG_SIZE, use_motion=True, aug=True):\n",
    "        self.records = records\n",
    "        self.t = t\n",
    "        self.img_size = img_size\n",
    "        self.use_motion = use_motion\n",
    "\n",
    "        # frame-level transforms (ImageNet)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "        # mild augmentation applied per frame\n",
    "        if aug:\n",
    "            self.aug = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "            ])\n",
    "        else:\n",
    "            self.aug = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vpath, label = self.records[idx]\n",
    "        clip = sample_motion_frames(vpath, k=self.t, img_size=self.img_size)\n",
    "        if clip is None:\n",
    "            # fallback: try re-read by uniform sampling of T indices\n",
    "            cap = cv2.VideoCapture(str(vpath))\n",
    "            n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            idxs = np.linspace(0, max(n-1,0), num=self.t).astype(int)\n",
    "            frames=[]\n",
    "            for i in idxs:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, int(i))\n",
    "                ok, f = cap.read()\n",
    "                if not ok: continue\n",
    "                f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "                f = cv2.resize(f, (self.img_size, self.img_size), interpolation=cv2.INTER_AREA)\n",
    "                frames.append(f.astype(\"float32\"))\n",
    "            cap.release()\n",
    "            if not frames:\n",
    "                raise RuntimeError(f\"Could not read frames from {vpath}\")\n",
    "            clip = np.stack(frames, axis=0)\n",
    "            if clip.shape[0] < self.t:\n",
    "                pad = np.repeat(clip[-1:], self.t-clip.shape[0], axis=0)\n",
    "                clip = np.concatenate([clip, pad], axis=0)\n",
    "\n",
    "        # HWC -> CHW per frame; stack to (T,3,H,W)\n",
    "        tensors = []\n",
    "        for f in clip:\n",
    "            img = f.astype(np.uint8)\n",
    "            if self.aug:  # aug expects PIL / Tensor; we’ll apply on tensor\n",
    "                x = self.to_tensor(img)\n",
    "                x = self.aug(x)\n",
    "            else:\n",
    "                x = self.to_tensor(img)\n",
    "            x = self.normalize(x)   # (3,H,W)\n",
    "            tensors.append(x)\n",
    "        xclip = torch.stack(tensors, dim=0)   # (T,3,H,W)\n",
    "        y = torch.tensor(label, dtype=torch.long)\n",
    "        return xclip, y\n",
    "\n",
    "ds_train = ClipDataset(train_rec, aug=True)\n",
    "ds_val   = ClipDataset(val_rec, aug=False)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "dl_val   = DataLoader(ds_val,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "len(ds_train), len(ds_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0cc3ffd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Apple\\AppData\\Local\\Temp\\ipykernel_10048\\4039676501.py:56: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5 (GRU version): Temporal model ---\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "\n",
    "class TemporalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2, backbone_name=\"mobilenet_v3_small\",\n",
    "                 hidden=128, bidirectional=True):\n",
    "        super().__init__()\n",
    "        # Frame encoder (shared)\n",
    "        if backbone_name == \"mobilenet_v3_small\":\n",
    "            m = torchvision.models.mobilenet_v3_small(\n",
    "                weights=torchvision.models.MobileNet_V3_Small_Weights.IMAGENET1K_V1\n",
    "            )\n",
    "            self.backbone = m.features              # conv body\n",
    "            feat_dim = 576                          # last conv channels\n",
    "        else:\n",
    "            raise ValueError(\"Backbone not supported here.\")\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Temporal module: BiGRU over per-frame features\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=feat_dim,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "        fc_in = hidden * (2 if bidirectional else 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(fc_in, num_classes)\n",
    "\n",
    "    def encode_frames(self, x):\n",
    "        # x: (B,T,3,H,W) -> (B,T,Cfeat)\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "        f = self.backbone(x)                # (B*T, C', h, w)\n",
    "        f = self.gap(f).squeeze(-1).squeeze(-1)  # (B*T, C')\n",
    "        f = f.view(B, T, -1)                # (B, T, C')\n",
    "        return f\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode frames, then GRU, then temporal mean-pooling over time\n",
    "        f = self.encode_frames(x)           # (B,T,Cfeat)\n",
    "        out, _ = self.gru(f)                # (B,T,hidden*dir)\n",
    "        f = out.mean(dim=1)                 # mean over time (robust + simple)\n",
    "        f = self.dropout(f)\n",
    "        return self.fc(f)\n",
    "\n",
    "model = TemporalClassifier(num_classes=len(CLASS_NAMES), hidden=128, bidirectional=True).to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR_WARMUP)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c7db372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Apple\\AppData\\Local\\Temp\\ipykernel_10048\\2742069223.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiny overfit acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 6 (GRU): Tiny overfit sanity on few clips ---\n",
    "tiny = min(24, len(ds_train))\n",
    "Xb, yb = [], []\n",
    "for i in range(tiny):\n",
    "    x,y = ds_train[i]\n",
    "    Xb.append(x); yb.append(y)\n",
    "Xb = torch.stack(Xb, dim=0).to(DEVICE)  # (tiny,T,3,H,W)\n",
    "yb = torch.stack(yb, dim=0).to(DEVICE)\n",
    "\n",
    "probe = TemporalClassifier(num_classes=2, hidden=128, bidirectional=True).to(DEVICE)\n",
    "opt = torch.optim.Adam(probe.parameters(), lr=1e-3)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "probe.train()\n",
    "for ep in range(15):\n",
    "    opt.zero_grad()\n",
    "    with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "        logits = probe(Xb)\n",
    "        loss = crit(logits, yb)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(probe.parameters(), max_norm=5.0)  # helps GRU stability\n",
    "    opt.step()\n",
    "\n",
    "acc = (logits.argmax(1) == yb).float().mean().item()\n",
    "print(\"Tiny overfit acc:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1ebd21f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[filter] kept: 598 | dropped: 0\n",
      "[filter] kept: 128 | dropped: 0\n",
      "[ready] train clips: 598 | val clips: 128 | batch=4 | workers=0\n",
      "batch shapes: (4, 8, 3, 224, 224) (4,)\n"
     ]
    }
   ],
   "source": [
    "# === CONSOLIDATED DATA PIPELINE (Windows-safe, motion clips, robust) ===\n",
    "# Drop this in once. It replaces your sampler, dataset, and dataloaders.\n",
    "\n",
    "import os, glob, random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "# ---------- knobs ----------\n",
    "T = 8                 # frames per clip (try 8–12 for more temporal context)\n",
    "IMG_SIZE = 224        # 224 for MobileNet; use 192 for speed if needed\n",
    "BATCH_SIZE = 4        # clips are heavy; 3–6 is typical on 8GB VRAM\n",
    "NUM_WORKERS = 0       # Windows/notebook-safe. Increase only in a .py script with if __name__ == \"__main__\"\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def is_video_ok(path):\n",
    "    cap = cv2.VideoCapture(str(path))\n",
    "    ok = cap.isOpened()\n",
    "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if ok else 0\n",
    "    cap.release()\n",
    "    return ok and n >= 2\n",
    "\n",
    "def filter_records(records):\n",
    "    good, bad = [], []\n",
    "    for vpath, cid in records:\n",
    "        if is_video_ok(vpath):\n",
    "            good.append((vpath, cid))\n",
    "        else:\n",
    "            bad.append(vpath)\n",
    "    print(f\"[filter] kept: {len(good)} | dropped: {len(bad)}\")\n",
    "    if bad: print(\"  dropped examples:\", bad[:5])\n",
    "    return good\n",
    "\n",
    "def sample_motion_frames(video_path, k=T, img_size=IMG_SIZE):\n",
    "    \"\"\"Pick the top-k frames by average frame-difference; return (T,H,W,3) float32 0..255.\"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened(): return None\n",
    "    n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    ok, prev = cap.read()\n",
    "    if not ok or n <= 1:\n",
    "        cap.release(); return None\n",
    "    prev_g = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
    "    frames = [prev]; diffs = []\n",
    "\n",
    "    for i in range(1, n):\n",
    "        ok, f = cap.read()\n",
    "        if not ok: break\n",
    "        frames.append(f)\n",
    "        g = cv2.cvtColor(f, cv2.COLOR_BGR2GRAY)\n",
    "        diffs.append((i, float(cv2.absdiff(g, prev_g).mean())))\n",
    "        prev_g = g\n",
    "    cap.release()\n",
    "    if not diffs: return None\n",
    "\n",
    "    k = min(k, len(diffs)) if diffs else 1\n",
    "    top = sorted(sorted(diffs, key=lambda x: x[1], reverse=True)[:k], key=lambda x: x[0])\n",
    "    idxs = [i for i, _ in top]\n",
    "\n",
    "    out = []\n",
    "    for i in idxs:\n",
    "        f = frames[i]\n",
    "        f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "        f = cv2.resize(f, (img_size, img_size), interpolation=cv2.INTER_AREA)\n",
    "        out.append(f.astype(\"float32\"))\n",
    "    if not out: return None\n",
    "    clip = np.stack(out, axis=0)  # (t,h,w,3)\n",
    "    if clip.shape[0] < T:  # pad to T\n",
    "        pad = np.repeat(clip[-1:], T - clip.shape[0], axis=0)\n",
    "        clip = np.concatenate([clip, pad], axis=0)\n",
    "    return clip\n",
    "\n",
    "class ClipDataset(Dataset):\n",
    "    \"\"\"Yields (T,3,H,W) normalized clips with robust loading/retries.\"\"\"\n",
    "    def __init__(self, records, t=T, img_size=IMG_SIZE, use_motion=True, aug=True, max_retries=3):\n",
    "        self.records = records\n",
    "        self.t = t\n",
    "        self.img_size = img_size\n",
    "        self.use_motion = use_motion\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "        self.aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "        ]) if aug else None\n",
    "\n",
    "    def __len__(self): return len(self.records)\n",
    "\n",
    "    def _read_clip(self, vpath):\n",
    "        clip = sample_motion_frames(vpath, k=self.t, img_size=self.img_size) if self.use_motion else None\n",
    "        if clip is None:\n",
    "            cap = cv2.VideoCapture(str(vpath))\n",
    "            if not cap.isOpened(): return None\n",
    "            n = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            if n <= 0:\n",
    "                cap.release(); return None\n",
    "            idxs = np.linspace(0, n - 1, num=self.t).astype(int)\n",
    "            frames = []\n",
    "            for i in idxs:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, int(i))\n",
    "                ok, f = cap.read()\n",
    "                if not ok or f is None: continue\n",
    "                f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "                f = cv2.resize(f, (self.img_size, self.img_size), interpolation=cv2.INTER_AREA)\n",
    "                frames.append(f.astype(\"float32\"))\n",
    "            cap.release()\n",
    "            if not frames: return None\n",
    "            clip = np.stack(frames, axis=0)\n",
    "        if clip.shape[0] < self.t:\n",
    "            pad = np.repeat(clip[-1:], self.t - clip.shape[0], axis=0)\n",
    "            clip = np.concatenate([clip, pad], axis=0)\n",
    "        return clip\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tries = 0\n",
    "        while tries < self.max_retries:\n",
    "            vpath, label = self.records[idx]\n",
    "            try:\n",
    "                clip = self._read_clip(vpath)\n",
    "                if clip is None: raise RuntimeError(\"empty clip\")\n",
    "                frames = []\n",
    "                for f in clip:\n",
    "                    x = self.to_tensor(f.astype(np.uint8))  # [0,1]\n",
    "                    if self.aug: x = self.aug(x)\n",
    "                    x = self.normalize(x)                   # ImageNet norm\n",
    "                    frames.append(x)\n",
    "                xclip = torch.stack(frames, dim=0)          # (T,3,H,W)\n",
    "                return xclip, torch.tensor(label, dtype=torch.long)\n",
    "            except Exception:\n",
    "                tries += 1\n",
    "                idx = random.randrange(len(self.records))  # try different sample\n",
    "        # final fallback: zero clip\n",
    "        xclip = torch.zeros((self.t, 3, self.img_size, self.img_size), dtype=torch.float32)\n",
    "        return xclip, torch.tensor(self.records[0][1], dtype=torch.long)\n",
    "\n",
    "def build_dataloaders(train_rec, val_rec):\n",
    "    # filter bad videos first\n",
    "    train_rec = filter_records(train_rec)\n",
    "    val_rec   = filter_records(val_rec)\n",
    "\n",
    "    ds_train = ClipDataset(train_rec, aug=True, use_motion=True)\n",
    "    ds_val   = ClipDataset(val_rec,   aug=False, use_motion=True)\n",
    "\n",
    "    dl_train = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "    dl_val   = DataLoader(ds_val,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "    return ds_train, ds_val, dl_train, dl_val\n",
    "\n",
    "# Build dataloaders (expects you already defined train_rec / val_rec earlier)\n",
    "ds_train, ds_val, dl_train, dl_val = build_dataloaders(train_rec, val_rec)\n",
    "\n",
    "print(f\"[ready] train clips: {len(ds_train)} | val clips: {len(ds_val)} | \"\n",
    "      f\"batch={BATCH_SIZE} | workers={NUM_WORKERS}\")\n",
    "\n",
    "# quick smoke test: load one batch\n",
    "try:\n",
    "    xb, yb = next(iter(dl_train))\n",
    "    print(\"batch shapes:\", tuple(xb.shape), tuple(yb.shape))  # expect (B,T,3,H,W), (B,)\n",
    "except Exception as e:\n",
    "    print(\"loader smoke test failed:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "efddc19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA DIR: C:\\Users\\Apple\\Desktop\\CV_last\\data\n",
      "class 'non_shop_lifters' videos: 531\n",
      "class 'shop_lifters' videos: 324\n",
      "SPLIT sizes | train/val/test: 598 128 129\n",
      "train counts: Counter({0: 371, 1: 227})\n",
      "val   counts: Counter({0: 80, 1: 48})\n",
      "test  counts: Counter({0: 80, 1: 49})\n"
     ]
    }
   ],
   "source": [
    "# --- Sanity A: basic counts and imbalance checks ---\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import os, glob\n",
    "\n",
    "def class_counts(recs):\n",
    "    return Counter([c for _, c in recs])\n",
    "\n",
    "def peek_paths(recs, k=3):\n",
    "    return [p for p,_ in recs[:k]]\n",
    "\n",
    "print(\"DATA DIR:\", DATA_DIR.resolve())\n",
    "for cname in CLASS_NAMES:\n",
    "    n = len(glob.glob(str(DATA_DIR / cname / \"*.mp4\")))\n",
    "    print(f\"class '{cname}' videos: {n}\")\n",
    "\n",
    "print(\"SPLIT sizes | train/val/test:\", len(train_rec), len(val_rec), len(test_rec))\n",
    "print(\"train counts:\", class_counts(train_rec))\n",
    "print(\"val   counts:\", class_counts(val_rec))\n",
    "print(\"test  counts:\", class_counts(test_rec))\n",
    "\n",
    "assert len(train_rec) and len(val_rec), \"Empty split!\"\n",
    "assert set([c for _,c in train_rec]) == {0,1}, \"Train split missing a class!\"\n",
    "assert set([c for _,c in val_rec])   == {0,1}, \"Val split missing a class!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7439129c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class mapping OK: ['non_shop_lifters', 'shop_lifters']\n"
     ]
    }
   ],
   "source": [
    "# --- Sanity B: save & reload the CLASS_NAMES immediately ---\n",
    "import json\n",
    "(MODELS_DIR).mkdir(exist_ok=True, parents=True)\n",
    "with open(MODELS_DIR / \"class_names.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(CLASS_NAMES, f)\n",
    "\n",
    "with open(MODELS_DIR / \"class_names.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded = json.load(f)\n",
    "assert loaded == CLASS_NAMES, f\"Class name mismatch! {loaded} vs {CLASS_NAMES}\"\n",
    "print(\"Class mapping OK:\", loaded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83bba9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: (4, 8, 3, 224, 224) labels: [1, 1, 1, 1]\n",
      "per-batch std: 1.0036391019821167\n"
     ]
    }
   ],
   "source": [
    "# --- Sanity C: loader smoke & clip stats ---\n",
    "xb, yb = next(iter(dl_train))\n",
    "print(\"batch:\", tuple(xb.shape), \"labels:\", yb.tolist())\n",
    "print(\"per-batch std:\", float(xb.std()))\n",
    "assert float(xb.std()) > 0.01, \"Clips look constant/blank; check sampling/paths/codecs.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2dfebc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights: [0.8059298992156982, 1.3171806335449219]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Apple\\AppData\\Local\\Temp\\ipykernel_10048\\2342574675.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
      "C:\\Users\\Apple\\AppData\\Local\\Temp\\ipykernel_10048\\2342574675.py:27: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 0.532479\n",
      "val = 0.882812\n",
      "train = 0.652174\n",
      "loss = 0.365017\n",
      "val = 0.875000\n",
      "train = 0.759197\n",
      "loss = 0.258715\n",
      "val = 0.914062\n",
      "train = 0.794314\n",
      "loss = 0.237759\n",
      "val = 0.906250\n",
      "train = 0.806020\n",
      "loss = 0.206483\n",
      "val = 0.921875\n",
      "train = 0.814381\n",
      "loss = 0.221573\n",
      "val = 0.906250\n",
      "train = 0.795987\n",
      "loss = 0.116287\n",
      "val = 0.960938\n",
      "train = 0.891304\n",
      "loss = 0.047598\n",
      "val = 0.992188\n",
      "train = 0.921405\n",
      "loss = 0.008955\n",
      "val = 1.000000\n",
      "train = 0.963211\n",
      "loss = 0.003672\n",
      "val = 1.000000\n",
      "train = 0.976589\n",
      "loss = 0.002440\n",
      "val = 1.000000\n",
      "train = 0.973244\n",
      "loss = 0.003858\n",
      "val = 1.000000\n",
      "train = 0.976589\n",
      "loss = 0.001004\n",
      "val = 1.000000\n",
      "train = 0.969900\n",
      "loss = 0.000572\n",
      "val = 1.000000\n",
      "train = 0.979933\n",
      "Best val acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "# --- Training with requested prints, proper warmup (frozen backbone) + finetune (last 3 blocks) ---\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# ===== 0) Class weights (helps if imbalanced) =====\n",
    "y_train = np.array([c for _, c in train_rec])\n",
    "cls_w = compute_class_weight(class_weight=\"balanced\", classes=np.array([0,1]), y=y_train)\n",
    "cls_w = torch.tensor(cls_w, dtype=torch.float32).to(DEVICE)\n",
    "print(\"class weights:\", cls_w.tolist())\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=cls_w)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "def set_trainable(module, flag: bool):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "def run_epoch(dl, model, optimizer=None):\n",
    "    train = optimizer is not None\n",
    "    model.train(train)\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for xb, yb in dl:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            # GRU stability + general safety\n",
    "            scaler.unscale_(optimizer)\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        correct += (logits.argmax(1) == yb).sum().item()\n",
    "        total += xb.size(0)\n",
    "\n",
    "    return total_loss/total, correct/total\n",
    "\n",
    "# ===== 1) WARMUP: freeze ALL backbone, train only GRU + FC =====\n",
    "set_trainable(model.backbone, False)     # <- freeze CNN\n",
    "set_trainable(model.gru, True)\n",
    "set_trainable(model.fc, True)\n",
    "set_trainable(model.gap, True)           # GAP has no params but harmless\n",
    "\n",
    "# Optimizer: only head params\n",
    "head_params = list(model.gru.parameters()) + list(model.fc.parameters())\n",
    "optimizer = torch.optim.Adam(head_params, lr=LR_WARMUP)\n",
    "\n",
    "best_val = 0.0\n",
    "for epoch in range(1, EPOCHS_WARMUP+1):\n",
    "    tr_loss, tr_acc = run_epoch(dl_train, model, optimizer)\n",
    "    va_loss, va_acc = run_epoch(dl_val,   model, optimizer=None)\n",
    "\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        torch.save(model.state_dict(), MODELS_DIR / \"shoplift_torch_state.pth\")\n",
    "\n",
    "    # EXACT format you requested\n",
    "    print(f\"loss = {va_loss:.6f}\")\n",
    "    print(f\"val = {va_acc:.6f}\")\n",
    "    print(f\"train = {tr_acc:.6f}\")\n",
    "\n",
    "# ===== 2) FINETUNE: unfreeze last 3 MobileNet blocks (keep early layers frozen) =====\n",
    "# Freeze all first\n",
    "set_trainable(model.backbone, False)\n",
    "# Then unfreeze last 3 blocks\n",
    "for i in range(len(model.backbone) - 3, len(model.backbone)):\n",
    "    set_trainable(model.backbone[i], True)\n",
    "\n",
    "# Keep GRU/FC trainable\n",
    "set_trainable(model.gru, True)\n",
    "set_trainable(model.fc, True)\n",
    "\n",
    "# Discriminative learning rates:\n",
    "#   - lower LR for (newly) unfrozen CNN blocks\n",
    "#   - higher LR for temporal head (GRU/FC)\n",
    "backbone_ft_params = []\n",
    "for i in range(len(model.backbone) - 3, len(model.backbone)):\n",
    "    backbone_ft_params += list(model.backbone[i].parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": backbone_ft_params, \"lr\": LR_FT * 0.5},   # smaller LR for conv\n",
    "    {\"params\": model.gru.parameters(), \"lr\": LR_FT},     # higher LR for head\n",
    "    {\"params\": model.fc.parameters(), \"lr\": LR_FT},\n",
    "])\n",
    "\n",
    "for epoch in range(1, EPOCHS_FT+1):\n",
    "    tr_loss, tr_acc = run_epoch(dl_train, model, optimizer)\n",
    "    va_loss, va_acc = run_epoch(dl_val,   model, optimizer=None)\n",
    "\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        torch.save(model.state_dict(), MODELS_DIR / \"shoplift_torch_state.pth\")\n",
    "\n",
    "    # EXACT format again\n",
    "    print(f\"loss = {va_loss:.6f}\")\n",
    "    print(f\"val = {va_acc:.6f}\")\n",
    "    print(f\"train = {tr_acc:.6f}\")\n",
    "\n",
    "print(\"Best val acc:\", best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee8d9fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "non_shop_lifters       0.99      1.00      0.99        80\n",
      "    shop_lifters       1.00      0.98      0.99        49\n",
      "\n",
      "        accuracy                           0.99       129\n",
      "       macro avg       0.99      0.99      0.99       129\n",
      "    weighted avg       0.99      0.99      0.99       129\n",
      "\n",
      "Confusion matrix:\n",
      " [[80  0]\n",
      " [ 1 48]]\n"
     ]
    }
   ],
   "source": [
    "# --- Robust video-level evaluation on test set ---\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "model.load_state_dict(torch.load(MODELS_DIR / \"shoplift_torch_state.pth\", map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "def predict_video(video_path):\n",
    "    clip = sample_motion_frames(video_path, k=T, img_size=IMG_SIZE)\n",
    "    if clip is None: return None\n",
    "    xb = clip_to_tensor(clip).unsqueeze(0).to(DEVICE)  # (1,T,3,H,W)\n",
    "    with torch.no_grad():\n",
    "        logits = model(xb)\n",
    "        prob = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "    return int(prob.argmax()), prob\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "for vpath, cid in test_rec:\n",
    "    res = predict_video(vpath)\n",
    "    if res is None: continue\n",
    "    pid, _ = res\n",
    "    y_true.append(cid); y_pred.append(pid)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff4e6ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "non_shop_lifters       0.99      1.00      0.99        80\n",
      "    shop_lifters       1.00      0.98      0.99        49\n",
      "\n",
      "        accuracy                           0.99       129\n",
      "       macro avg       0.99      0.99      0.99       129\n",
      "    weighted avg       0.99      0.99      0.99       129\n",
      "\n",
      "Confusion matrix:\n",
      " [[80  0]\n",
      " [ 1 48]]\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 8: Video-level evaluation ---\n",
    "softmax = nn.Softmax(dim=1).to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODELS_DIR / \"shoplift_torch_state.pth\", map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "def predict_video(video_path):\n",
    "    clip = sample_motion_frames(video_path, k=T, img_size=IMG_SIZE)\n",
    "    if clip is None: return None\n",
    "    # (T,H,W,3) -> (1,T,3,H,W)\n",
    "    tlist = []\n",
    "    for f in clip:\n",
    "        x = transforms.functional.to_tensor(f.astype(np.uint8))\n",
    "        x = transforms.functional.normalize(x, IMAGENET_MEAN, IMAGENET_STD)\n",
    "        tlist.append(x)\n",
    "    xb = torch.stack(tlist, dim=0).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(xb)\n",
    "        prob = softmax(logits)[0].cpu().numpy()  # (2,)\n",
    "    pred = int(np.argmax(prob))\n",
    "    return pred, prob\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "for vpath, cid in test_rec:\n",
    "    res = predict_video(vpath)\n",
    "    if res is None: continue\n",
    "    pid, _ = res\n",
    "    y_true.append(cid); y_pred.append(pid)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_true, y_pred, target_names=CLASS_NAMES))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b1ba433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: models\\shoplift_torch_state.pth\n",
      "Saved: models\\class_names.json\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 9: Save class names (PyTorch) ---\n",
    "with open(MODELS_DIR / \"class_names.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(CLASS_NAMES, f)\n",
    "print(\"Saved:\", MODELS_DIR / \"shoplift_torch_state.pth\")\n",
    "print(\"Saved:\", MODELS_DIR / \"class_names.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
